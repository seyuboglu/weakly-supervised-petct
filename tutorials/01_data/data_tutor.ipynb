{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Tutorial\n",
    "Contacts: eyuboglu@stanford.edu, gangus@stanford.edu\n",
    "\n",
    "How to prepare data for our multi-task, weak supervision framework.  We use  [HDF5](https://www.hdfgroup.org/solutions/hdf5/), a file format optimized for high-dimensional heterogeneous datasets (like ours!). Using HDF5 allows us to store all of the data (i.e. scan, report, and metadata) for each exam in our dataset in one place. Unfortunately, many of you are probably unfamiliar with the HDF5 interface, so below we walk you through how to prepare an HDF5 dataset with volumetric imaging data for use in our framework!\n",
    "\n",
    "In this notebook we:  \n",
    "\n",
    "1. Go through the motions of preparing an HDF5 dataset for use with our framework. We prepare the dataset with dummy data, but show how you can replace a few functions with custom ones for loading your own data. \n",
    "\n",
    "2. Show how we can use this HDF5 dataset with the PyTorch `Dataset` classes we've implemented such as `pet_ct.learn.datasets.MTClassifierDataset`. (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import various packages. Make sure you're in an environment with the `pet_ct` package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# import requirements\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to package directory\n",
    "os.chdir(\"/Users/sabrieyuboglu/Documents/sabri/research/projects/fdg-pet-ct/pet-ct\")\n",
    "\n",
    "experiment_dir = \"tutorials/01_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Creating a Toy HDF5 Dataset\n",
    "\n",
    "The dataloading interface we provide in pet_ct.learn.datasets expect data stored in a particular format within an [HDF5](https://www.hdfgroup.org/solutions/hdf5/) file. Below we walk through the process of generating a HDF5 file for use in our framework. We generate random data to fill the dataset with the `get_toy_exam` method below, but if you'd like to use your own data, simply implement the `get_exam` method below to fetch a volumetric imaging exam in your dataset. \n",
    "\n",
    "It's also worth noting that we use the [`h5py` package](https://www.h5py.org/), a Pythonic interface to the HDF5 file format. If you want to dive into the documentation for any of the h5py functions and classes we use below check out the [documentation](http://docs.h5py.org/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toy_exam(): \n",
    "    \"\"\" Generates a simulated PET/CT exam with random\n",
    "    data.\n",
    "    \"\"\"       \n",
    "    num_slices = np.random.randint(low=20, high=30)\n",
    "    ct_images = np.random.normal(size=(num_slices, 512, 512))\n",
    "    pet_images = np.random.normal(size=(num_slices, 128, 128))\n",
    "    report = \"IMPRESSION: This is a toy report.\"\n",
    "    return ct_images, pet_images, report\n",
    "\n",
    "## TODO: Implement a get_exam method to work with your data\n",
    "def get_exam(exam_path: str):\n",
    "    \"\"\" Write your own method for fetching an imaging exam in your\n",
    "    dataset.\n",
    "    args:\n",
    "        exam_path (str): the path to the exam data in your filesystem\n",
    "    returns:\n",
    "        ct_images (np.ndarray): a numpy array of shape (num_slices, 512, 512)\n",
    "            where num_slices can be variable. \n",
    "        pet_images (np.ndarray): a numpy array of shape (num_slices, 128, 128)\n",
    "            where num_slices can be variable. \n",
    "    \"\"\"\n",
    "    ct_images = None  #  (num_slices, 512, 512)\n",
    "    pet_images = None  # (num_slices, 128, 128)\n",
    "    return ct_images, pet_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tutorials/01_data/toy_dataset/exams.csv`: We've built a CSV of toy exams indexed by \"exam_id\", a unique identifier for each exam, and with a column \"exam_path\" specifying where in the filesystem the data is stored. \n",
    "\n",
    "TODO: Build a CSV for your data with matching format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                    exam_path  patient_id\nexam_id                                                  \nxid6412379  data/toy_dataset/exams/xid6412379  pid8389665\nxid1706528  data/toy_dataset/exams/xid1706528  pid5241154\nxid3418793  data/toy_dataset/exams/xid3418793  pid7287107\nxid9559466  data/toy_dataset/exams/xid9559466  pid2357355\nxid8868720  data/toy_dataset/exams/xid8868720  pid7656390\n...                                       ...         ...\nxid2711892  data/toy_dataset/exams/xid2711892  pid2306946\nxid9200258  data/toy_dataset/exams/xid9200258  pid7468753\nxid9725040  data/toy_dataset/exams/xid9725040  pid2414282\nxid7079800  data/toy_dataset/exams/xid7079800  pid9483594\nxid1790406  data/toy_dataset/exams/xid1790406  pid9001438\n\n[100 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>exam_path</th>\n      <th>patient_id</th>\n    </tr>\n    <tr>\n      <th>exam_id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>xid6412379</th>\n      <td>data/toy_dataset/exams/xid6412379</td>\n      <td>pid8389665</td>\n    </tr>\n    <tr>\n      <th>xid1706528</th>\n      <td>data/toy_dataset/exams/xid1706528</td>\n      <td>pid5241154</td>\n    </tr>\n    <tr>\n      <th>xid3418793</th>\n      <td>data/toy_dataset/exams/xid3418793</td>\n      <td>pid7287107</td>\n    </tr>\n    <tr>\n      <th>xid9559466</th>\n      <td>data/toy_dataset/exams/xid9559466</td>\n      <td>pid2357355</td>\n    </tr>\n    <tr>\n      <th>xid8868720</th>\n      <td>data/toy_dataset/exams/xid8868720</td>\n      <td>pid7656390</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>xid2711892</th>\n      <td>data/toy_dataset/exams/xid2711892</td>\n      <td>pid2306946</td>\n    </tr>\n    <tr>\n      <th>xid9200258</th>\n      <td>data/toy_dataset/exams/xid9200258</td>\n      <td>pid7468753</td>\n    </tr>\n    <tr>\n      <th>xid9725040</th>\n      <td>data/toy_dataset/exams/xid9725040</td>\n      <td>pid2414282</td>\n    </tr>\n    <tr>\n      <th>xid7079800</th>\n      <td>data/toy_dataset/exams/xid7079800</td>\n      <td>pid9483594</td>\n    </tr>\n    <tr>\n      <th>xid1790406</th>\n      <td>data/toy_dataset/exams/xid1790406</td>\n      <td>pid9001438</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "exams_df = pd.read_csv(\"tutorials/01_data/toy_dataset/exams.csv\", index_col=0)\n",
    "from IPython.display import display\n",
    "with pd.option_context('display.max_rows', 10):\n",
    "    display(exams_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "dataset_path = \"data/toy_dataset/data.hdf5\"\n",
    "file = h5py.File(dataset_path, 'a')\n",
    "file.create_group(\"exams\")\n",
    "exams = file[\"exams\"]\n",
    "\n",
    "for exam_id, exam in exams_df.iterrows():\n",
    "    ct_images, pet_images, report = get_toy_exam()\n",
    "\n",
    "    exams.create_group(exam_id)\n",
    "    # note: we use one HDF5 dataset for each exam in our dataset, hence the\n",
    "    # the perhaps confusing function name `create_dataset`\n",
    "    exams[exam_id].create_dataset(\"ct\", data=ct_images)\n",
    "    exams[exam_id].create_dataset(\"pet\", data=pet_images)\n",
    "    exams[exam_id].create_dataset(\"report\", \n",
    "                                          data=np.string_(report), \n",
    "                                          dtype=h5py.special_dtype(vlen=str))\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs.size -> torch.Size([1, 205, 224, 224, 2])\n",
    "                           ^   ^    ^    ^   ^\n",
    "                          (0) (1)  (2)  (3) (4)\n",
    "(0) batch size\n",
    "(1) number of slices in scan, bottom (feet) -> top (head) \n",
    "(2) height \n",
    "(3) width\n",
    "(4) two channels: one for PET one for CT "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output[\"liver\"] -> tensor([[0.9661, 0.0339]], ... )\n",
    "                              ^        ^\n",
    "                             (0)      (1)\n",
    "(0) probability there is NO abnormality in the liver\n",
    "(1) probability of abnormality in the liver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('env': venv)",
   "language": "python",
   "name": "python37364bitenvvenvd90de790887a4a1fba07ad273bc876d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "791px",
    "left": "1428px",
    "top": "110px",
    "width": "252px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}