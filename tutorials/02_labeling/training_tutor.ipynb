{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PET-CT Inference Tutorial\n",
    "Contacts: eyuboglu@stanford.edu, gangus@stanford.edu\n",
    "\n",
    "How to perform inference on the task of abnormality localization with a pretrained scan model.  \n",
    "\n",
    "In this notebook we cover:  \n",
    "1. Loading model configurations from a JSON like the one at `tutorials/inference/params.json`  \n",
    "2. Building a `pet_ct.model.MTClassifierModel` and loading pretrained weights (Note: we do not provide pretrained weights for our models to protect PHI.)  \n",
    "3. How input to the model should be structured  \n",
    "4. How to perform inference on the model using   `pet_ct.model.MTClassifierModel.score`  \n",
    "5. How output is structured "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import various packages. Make sure you're in an environment with the `pet_ct` package installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# import requirements\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "import pet_ct.model.models as models\n",
    "from pet_ct.model.classifier_model import MTClassifierModel\n",
    "from pet_ct.learn.datasets import MTClassifierDataset\n",
    "from pet_ct.learn.dataloaders import MTExamDataLoader\n",
    "from pet_ct.util.util import set_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to package directory\n",
    "os.chdir(\"/Users/sabrieyuboglu/Documents/sabri/research/projects/fdg-pet-ct/pet-ct\")\n",
    "\n",
    "experiment_dir = \"tutorials/inference\"\n",
    "set_logger(log_path=os.path.join(experiment_dir, \"process.log\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select your CUDA devices if available\n",
    "devices = []\n",
    "cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading hyper-parameters\n",
    "We've included a params file at `notebooks/tutorial/params.json`. Please take a quick look at it toget a sense of its structure and what we include in the params\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = MTClassifierModel.load_params(os.path.join(experiment_dir, \"params.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model\n",
    "Let's use the parameters we've loaded to build a model. We'll also load pretrained weights from `notebooks/tutorial/weights.tar`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_class, model_args, weights_path=None):\n",
    "    model_class = getattr(models, model_class)\n",
    "    model = model_class(cuda=cuda, devices=devices, **model_args)\n",
    "    if weights_path is not None: \n",
    "        model.load_weights(weights_path, device=devices[0])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model and load weights, you should see 550/550 pretrained params loaded. \n",
    "model = build_model(params[\"model_class\"], \n",
    "                    params[\"model_args\"],\n",
    "                    os.path.join(experiment_dir, \"weights.tar\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to structure inputs?\n",
    "To understand how to structure inputs properly we will load some training examples from our dataset. However, the `MTClassifierDataset` class below is designed for data in our databases at Stanford. You'll likely need to write your own dataset classes for your data. You should use `MTClassifierDataset` as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this building this dataset will likely not work for you \n",
    "# because you don't have access to our data. \n",
    "# We do so here simply to demonstrate the structure of the data.\n",
    "dataset = MTClassifierDataset(**params[\"dataset_args\"], split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = MTExamDataLoader(dataset=dataset, \n",
    "                              num_workers=1, \n",
    "                              batch_size=1,\n",
    "                              sampler=\"RandomSampler\",\n",
    "                              num_samples=200)\n",
    "iterator = iter(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an example from the dataloader and examine its structure. Each PET-CT exam is represented by a torch tensor with 4 axes. There's an additional axis for the mini-batch. Its important that your input to the model also match this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets, info = iterator.next()\n",
    "print(f\"Input shape: {inputs.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "inputs.size -> torch.Size([1, 205, 224, 224, 2])\n",
    "                           ^   ^    ^    ^   ^\n",
    "                          (0) (1)  (2)  (3) (4)\n",
    "(0) batch size\n",
    "(1) number of slices in scan, bottom (feet) -> top (head) \n",
    "(2) height \n",
    "(3) width\n",
    "(4) two channels: one for PET one for CT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to make a prediction?\n",
    "Let's pass the inputs through the model using the `model.predict` function and examine the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.predict(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How is output structured?\n",
    "Let's examine what the model output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Output is of type: {type(output)}.\")\n",
    "print(f\"The keys of the dict are: {output.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Key:* The model outputs a **dictionary** with keys corresponding to each **task**.\n",
    "The keys map to the predictions for the task. Let's take a look at the output for the `liver` task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"liver\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output[\"liver\"] -> tensor([[0.9661, 0.0339]], ... )\n",
    "                              ^        ^\n",
    "                             (0)      (1)\n",
    "(0) probability there is NO abnormality in the liver\n",
    "(1) probability of abnormality in the liver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **targets** (i.e. labels) that we loaded before have a very similar structure as the output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"targets is of type: {type(targets)}.\")\n",
    "print(f\"The keys of the dict are: {targets.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[\"liver\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the target for the liver match the output of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to score the model on a dataset of examples?\n",
    "What if we want to evaluate the model on a dataset of examples? For this we can use the `model.score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_configs = [{'fn': 'accuracy'},\n",
    "           {'fn': 'roc_auc'},\n",
    "           {'fn': 'recall'},\n",
    "           {'fn': 'precision'},\n",
    "           {'fn': 'f1_score'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.score(dataloader, metric_configs=metric_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look and see how the model did for this particular subset of the test set on each of the tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('env': venv)",
   "language": "python",
   "name": "python37364bitenvvenvd90de790887a4a1fba07ad273bc876d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "791px",
    "left": "1428px",
    "top": "110px",
    "width": "252px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
